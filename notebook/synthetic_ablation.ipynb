{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417a3e63-fe29-46e0-bfc2-3b9a2f8df7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44936207-f4f4-4440-af07-69463a44b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VOCDetection\n",
    "from xml.etree.ElementTree import parse as ET_parse\n",
    "from utils.crowd.synthetic_data import generate_random_conf_matrix, generate_dl_conf_matrix, generate_box_parameters, generate_synthetic_data, xyxy2xywh\n",
    "from models.rpn_generator import RPNGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d48b49-ee7d-4b7e-878c-2b9766fa6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVOCDetection(VOCDetection):\n",
    "    classes = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.build_gt()\n",
    "    \n",
    "    def build_gt(self):\n",
    "        # convert to [xmin, ymin, xmax, ymax, class_id]\n",
    "        targets = []\n",
    "        for index in range(len(self.annotations)):\n",
    "            target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())['annotation']['object']\n",
    "            target = [[x['bndbox']['xmin'], x['bndbox']['ymin'], x['bndbox']['xmax'], x['bndbox']['ymax'], self.classes.index(x['name'])]\n",
    "                      for x in target if x['difficult'] == '0']\n",
    "            target = [list(map(int, x)) for x in target]\n",
    "            targets.append(target)\n",
    "        self.gt = targets\n",
    "    \n",
    "    def get_gt(self, index: int):        \n",
    "        return self.gt[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871b7b15-c32e-4fe4-9b12-69f24d39c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as FT\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "\n",
    "\n",
    "class StandardTransform:\n",
    "    def __init__(self, image_size=512, augment=[]):\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __call__(self, img, bbox):\n",
    "        new_image = FT.to_tensor(img)\n",
    "        new_bbox = torch.FloatTensor(bbox)\n",
    "        for augment in self.augment:\n",
    "            new_image, new_bbox = self.augment(new_bbox)\n",
    "        \n",
    "        # resize image and box\n",
    "        dims = (self.image_size, self.image_size)\n",
    "        new_image = FT.resize(new_image, dims)\n",
    "\n",
    "        # Resize bounding boxes\n",
    "        old_dims = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n",
    "        new_bbox[:, :4] = new_bbox[:, :4] / old_dims  # percent coordinates\n",
    "\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_bbox[:, :4] = new_bbox[:, :4] * new_dims\n",
    "\n",
    "        return new_image, new_bbox\n",
    "    \n",
    "    \n",
    "def download_voc_2007(root='../.data/'):\n",
    "    voc_train = CustomVOCDetection(root=root, year='2007', image_set='trainval', download=True)\n",
    "    # voc_val = CustomVOCDetection(root=root, year='2007', image_set='val', download=True)\n",
    "    voc_test = CustomVOCDetection(root=root, year='2007', image_set='test', download=True)\n",
    "    return voc_train, voc_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17525e0-f554-4920-97e1-8f3d0f71c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../.data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ../.data/VOCtrainval_06-Nov-2007.tar to ../.data/\n",
      "Using downloaded and verified file: ../.data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ../.data/VOCtest_06-Nov-2007.tar to ../.data/\n"
     ]
    }
   ],
   "source": [
    "voc_train, voc_test = download_voc_2007()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f527424-27f9-4e1b-be8a-a692adb752c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5011, 4952)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train), len(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf68b30-4e49-4ca2-b6ed-6b0bb4cfa8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_classes = ['background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "voc_classes = ['background', 'airplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'dining table', 'dog', 'horse',\n",
    "               'motorcycle', 'person', 'potted plant',\n",
    "               'sheep', 'couch', 'train', 'tv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0495aa2a-301b-473f-baf4-a37528d646d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco2voc_classes = [coco_classes.index(c) for c in voc_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759f236a-fb57-4c9f-a8c8-91413f859bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_cm = np.load('../outputs/classification-coco/alexnet_scratch/conf_matrix.npy')[coco2voc_classes][:, coco2voc_classes] + 1\n",
    "vgg16_cm = np.load('../outputs/classification-coco/vgg16_scratch/conf_matrix.npy')[coco2voc_classes][:, coco2voc_classes] + 1\n",
    "resnet50_cm = np.load('../outputs/classification-coco/resnet50_scratch/conf_matrix.npy')[coco2voc_classes][:, coco2voc_classes] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2245c4cd-4463-40df-b605-488cf0360c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(voc_train, shuffle=False, batch_size=16, collate_fn=lambda x: x)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4aead37-3c90-45b5-a2cb-a93393d6a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [02:17<00:00,  2.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [04:33<00:00,  1.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [06:16<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# rpn proposals\n",
    "resnet18_weight = '../outputs/rpn-coco/resnet18/weights/epoch_004.pt'\n",
    "resnet50_weight = '../outputs/rpn-coco/resnet50/weights/epoch_004.pt'\n",
    "resnet101_weight = '../outputs/rpn-coco/resnet101/weights/epoch_004.pt'\n",
    "resnet18_proposals, resnet50_proposals, resnet101_proposals = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for weight, results in zip([resnet18_weight, resnet50_weight, resnet101_weight], [resnet18_proposals, resnet50_proposals, resnet101_proposals]):\n",
    "        model = torch.load(weight)['model']\n",
    "        model.rpn.nms_thresh = 0.9\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        for data in tqdm(dataloader, total=len(dataloader)):\n",
    "            imgs = list(FT.to_tensor(x[0]).to(device) for x in data)\n",
    "            proposals, scores, _ = model(imgs)\n",
    "            for proposal, score in zip(proposals, scores):\n",
    "                results.append(np.concatenate([proposal.cpu().numpy(), np.expand_dims(score.cpu().numpy(), -1)], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a51fcd-1923-4c5a-86a4-6583018b3545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_proposals[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016f0b3-e353-4075-bb1c-d704ef5c1b41",
   "metadata": {},
   "source": [
    "# Increasing K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3abc626b-0a3d-449c-b20b-2ca93b274eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_template = \"\"\"train: ./data/voc2007_abls/{}/noisy_train\n",
    "clean_train: ./data/voc2007_abls/voc2007_ann1_k/clean_train\n",
    "val: ./data/voc2007_abls/voc2007_ann1_k/test\n",
    "test: ./data/voc2007_abls/voc2007_ann1_k/test\n",
    "image_dir: ./.data/VOCdevkit/VOC2007/JPEGImages\n",
    "\n",
    "# number of classes\n",
    "nc: 20\n",
    "\n",
    "# number of classes by annotator\n",
    "nc_ann: 20\n",
    "\n",
    "# number of annotators\n",
    "n_annotator: {}\n",
    "\n",
    "earl_ann_weights: {}  # all weights are same\n",
    "\n",
    "# class names\n",
    "names: ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "        'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant',\n",
    "        'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "# augmentations settings\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "rotate: 10  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.1  # image scale (+/- gain)\n",
    "shear: 0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ff28f7-4e89-463c-9896-39123769fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic train dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def crowd_labels2df(crowd_labels, dataset):\n",
    "    temp_data = []\n",
    "    for crowd_label, img_file in zip(crowd_labels, dataset.images):\n",
    "        img_file = os.path.basename(img_file)\n",
    "        for ann_id, annotations in enumerate(crowd_label):\n",
    "            for box in annotations:\n",
    "                temp_data.append([img_file, *box, ann_id])\n",
    "\n",
    "    df = pd.DataFrame(temp_data, columns=['img_path', 'x1', 'y1', 'x2', 'y2', 'class_id', 'annotator_id'])\n",
    "    df['class_id'] = df['class_id'] - 1\n",
    "    # for x in ['x1', 'y1', 'x2', 'y2']:\n",
    "    #     df[x] = df[x] - 1\n",
    "    return df\n",
    "\n",
    "def crowd_labels_df2txt(df, path='../data/voc2007'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for filename, group in df.groupby('img_path'):\n",
    "        filename += '.txt'\n",
    "        group.drop(columns='img_path').to_csv(os.path.join(path, filename), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a536f1d6-35d5-40c7-a646-b383ed5d2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2df(dataset):\n",
    "    temp_data = []\n",
    "    for gt, img_file in zip(dataset.gt, dataset.images):\n",
    "        img_file = os.path.basename(img_file)\n",
    "        # print(gt)\n",
    "        for box in gt:\n",
    "            temp_data.append([img_file, *box])\n",
    "\n",
    "    df = pd.DataFrame(temp_data, columns=['img_path', 'x1', 'y1', 'x2', 'y2', 'class_id'])\n",
    "    df['class_id'] = df['class_id'] - 1\n",
    "    for x in ['x1', 'y1', 'x2', 'y2']:\n",
    "        df[x] = df[x] - 1\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c47fb40e-2c57-4851-a522-9e1df94a47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def df_to_cocojson(df, path='../data/voc2007'):\n",
    "    temp = []\n",
    "    id_ = 0\n",
    "    for filename, group in df.groupby('img_path'):\n",
    "        box_list = group.to_numpy()\n",
    "        box_list[:, 1:5] = xyxy2xywh(box_list[:, 1:5])\n",
    "        for box in box_list:\n",
    "            image_id = int(box[0].split('.')[0])\n",
    "            bbox = box[1:5]\n",
    "            \n",
    "            class_id = box[5]\n",
    "            temp.append({'image_id': image_id,\n",
    "                         'category_id': class_id,\n",
    "                         'bbox': [round(x, 3) for x in bbox],\n",
    "                         'iscrowd': 0, 'id': id_, 'area': box[2] * box[3]})\n",
    "            id_ += 1\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2fd9bc2-6662-41ea-82d6-0c2cc8673f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 2, 5, 10, 50, 100, 500, 1000]:\n",
    "    conf_matrix = generate_dl_conf_matrix(k, vgg16_cm)\n",
    "    rpn_proposals = {i: resnet50_proposals for i in range(k)}\n",
    "    crowd_labels, conf_matrix, box_params = generate_synthetic_data(voc_train, n_annotator=k, conf_matrix=conf_matrix, fixed_box=False,\n",
    "                                                                    rpn_proposals=rpn_proposals)\n",
    "    df = crowd_labels2df(crowd_labels, voc_train)\n",
    "    save_path = f'../data/voc2007_abls/voc2007_ann{k}_k'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.save(os.path.join(save_path, 'conf_matrix.npy'), conf_matrix)\n",
    "    crowd_labels_df2txt(df, path=os.path.join(save_path, 'noisy_train'))\n",
    "    with open(f'../data/voc2007_abls/voc_2007_ann{k}_k.yaml', 'w') as f:\n",
    "        f.write(yaml_template.format(f'voc2007_ann{k}_k', k, '[]'))\n",
    "    if k == 1:  # only save same df once to save disk space\n",
    "        train_clean_df = clean2df(voc_train)\n",
    "        crowd_labels_df2txt(train_clean_df, path=os.path.join(save_path, 'clean_train'))\n",
    "        test_df = clean2df(voc_test)\n",
    "        crowd_labels_df2txt(test_df, path=os.path.join(save_path, 'test'))\n",
    "        df_to_cocojson(test_df, os.path.join(save_path, 'instances_test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605b100-94c6-4fd3-9026-423a123dc865",
   "metadata": {},
   "source": [
    "# Decreasing Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92674f-2807-4ce5-9b30-99bd9077d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "seed = 1234\n",
    "for nr in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    n_expert = int((1.0 - nr) * k)\n",
    "    n_noise = k - n_expert\n",
    "    print(n_expert, n_noise, nr)\n",
    "    exp_conf_matrix = generate_dl_conf_matrix(n_expert, resnet50_cm, seed=seed)\n",
    "    noise_conf_matrix = generate_dl_conf_matrix(n_noise, alexnet_cm, seed=seed+1)\n",
    "    conf_matrix = np.concatenate((exp_conf_matrix, noise_conf_matrix), axis=0)\n",
    "    rpn_proposals = {}\n",
    "    for i in range(n_expert):\n",
    "        rpn_proposals[i] = resnet101_proposals\n",
    "    for i in range(n_expert, k):\n",
    "        rpn_proposals[i] = resnet18_proposals\n",
    "    assert len(rpn_proposals) == conf_matrix.shape[0]\n",
    "    crowd_labels, conf_matrix, box_params = generate_synthetic_data(voc_train, n_annotator=k, conf_matrix=conf_matrix, fixed_box=False,\n",
    "                                                                    rpn_proposals=rpn_proposals)\n",
    "    df = crowd_labels2df(crowd_labels, voc_train)\n",
    "    save_path = f'../data/voc2007_abls/voc2007_ann{k}_nr{nr:.1f}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.save(os.path.join(save_path, 'conf_matrix.npy'), conf_matrix)\n",
    "    crowd_labels_df2txt(df, path=os.path.join(save_path, 'noisy_train'))\n",
    "    earl_weights = [[n_expert, 0.7364], [k, 0.3465]]\n",
    "    with open(f'../data/voc2007_abls/voc_2007_ann{k}_nr{nr:.1f}.yaml', 'w') as f:\n",
    "        f.write(yaml_template.format(f'voc2007_ann{k}_nr{nr:.1f}', k, str(earl_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87d59b-56ed-40d6-88d6-2d1160fbb472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdc",
   "language": "python",
   "name": "bdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
